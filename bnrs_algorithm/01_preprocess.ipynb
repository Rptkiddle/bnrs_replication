{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bnrs_algorithm: preprocessing\n",
    "\n",
    "This notebook contains the preprocessing steps required to prepare the data for the BNRS recommendation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggested (conda) environment setup:\n",
    "\n",
    "```bash\n",
    "# Create base env with conda\n",
    "conda create -n bnrs_algorithm python=3.11 numpy pandas scikit-learn networkx tqdm nltk -c conda-forge\n",
    "conda activate bnrs_algorithm\n",
    "\n",
    "# Install PyTorch — CPU-only example:\n",
    "conda install pytorch-c pytorch\n",
    "\n",
    "# Install packages from PyPI\n",
    "pip install sentence-transformers keybert keyphrase-vectorizers spacy datasets\n",
    "\n",
    "# Install spaCy models used in the notebook\n",
    "python -m spacy download en_core_web_lg\n",
    "\n",
    "# Download NLTK stopwords\n",
    "python -c \"import nltk; nltk.download('stopwords')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rupertkiddle/miniconda3/envs/bnrs_algorithm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import torch\n",
    "import spacy\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#NOTE: we use a custom wrapper for KeyBERT to return embeddings alongside phrases and scores:\n",
    "#(this is in utils/keybert_return_embeds.py)\n",
    "from utils.keybert_return_embeds import KeyBERTEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nLoads news corpus for pre-processing. \\n\\nNote: This pre-processing workflow assumes your article data has already been cleaned using typical steps \\n(e.g., removing special characters, standardizing whitespace, handling missing values, etc.). \\nThe CSV file should contain 'title' and 'article' columns with the cleaned article text.\\n\\nIn this example, we load one random day of news articles from the 'all-the-news' dataset.\\nThe dataset is available at: https://huggingface.co/datasets/rjac/all-the-news-2-1-Component-one\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loads news corpus for pre-processing. \n",
    "\n",
    "Note: This pre-processing workflow assumes your article data has already been cleaned using typical steps \n",
    "(e.g., removing special characters, standardizing whitespace, handling missing values, etc.). \n",
    "The CSV file should contain 'title' and 'article' columns with the cleaned article text.\n",
    "\n",
    "In this example, we load one random day of news articles from the 'all-the-news' dataset.\n",
    "The dataset is available at: https://huggingface.co/datasets/rjac/all-the-news-2-1-Component-one\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download example dataset (AllTheNews2.1; ~8.8GB)\n",
    "full_dataset = load_dataset(\"rjac/all-the-news-2-1-Component-one\", split=\"train\", cache_dir=\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random row index: 772965\n",
      "Year: 2018\n",
      "Month: 4.0\n",
      "Day: 4\n"
     ]
    }
   ],
   "source": [
    "#randomly select a row index and extract year, month, day values\n",
    "random_index = random.randint(0, len(full_dataset) - 1)\n",
    "random_row = full_dataset[random_index]\n",
    "\n",
    "year, month, day = random_row['year'], random_row['month'], random_row['day']\n",
    "print(f\"Random row index: {random_index}\")\n",
    "print(f\"Year: {year}\")\n",
    "print(f\"Month: {month}\")\n",
    "print(f\"Day: {day}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching articles for 2018-4.0-4: 2052\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset to get rows matching those values\n",
    "filtered_dataset = full_dataset.filter(\n",
    "    lambda row: row['year'] == year and row['month'] == month and row['day'] == day\n",
    ")\n",
    "news_df = filtered_dataset.to_pandas()\n",
    "\n",
    "print(f\"Number of matching articles for {year}-{month}-{day}: {len(news_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles after dropping missing values: 2035\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>publication</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>docs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4768774509216352572</th>\n",
       "      <td>2018-04-04</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Brazil soy exporters set to win big from U.S.-...</td>\n",
       "      <td>SAO PAULO (Reuters) - China’s move to slap tar...</td>\n",
       "      <td>Brazil soy exporters set to win big from U.S.-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5644548799481183751</th>\n",
       "      <td>2018-04-04</td>\n",
       "      <td>Vox</td>\n",
       "      <td>Mueller: Trump’s not a target. 4 theories on w...</td>\n",
       "      <td>The new report that special counsel Robert Mue...</td>\n",
       "      <td>Mueller: Trump’s not a target. 4 theories on w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3432312715945854947</th>\n",
       "      <td>2018-04-04</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Trump to order National Guard to protect borde...</td>\n",
       "      <td>WASHINGTON (Reuters) - President Donald Trump ...</td>\n",
       "      <td>Trump to order National Guard to protect borde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           date publication  \\\n",
       "id                                            \n",
       "4768774509216352572  2018-04-04     Reuters   \n",
       "5644548799481183751  2018-04-04         Vox   \n",
       "3432312715945854947  2018-04-04     Reuters   \n",
       "\n",
       "                                                                 title  \\\n",
       "id                                                                       \n",
       "4768774509216352572  Brazil soy exporters set to win big from U.S.-...   \n",
       "5644548799481183751  Mueller: Trump’s not a target. 4 theories on w...   \n",
       "3432312715945854947  Trump to order National Guard to protect borde...   \n",
       "\n",
       "                                                               article  \\\n",
       "id                                                                       \n",
       "4768774509216352572  SAO PAULO (Reuters) - China’s move to slap tar...   \n",
       "5644548799481183751  The new report that special counsel Robert Mue...   \n",
       "3432312715945854947  WASHINGTON (Reuters) - President Donald Trump ...   \n",
       "\n",
       "                                                                  docs  \n",
       "id                                                                      \n",
       "4768774509216352572  Brazil soy exporters set to win big from U.S.-...  \n",
       "5644548799481183751  Mueller: Trump’s not a target. 4 theories on w...  \n",
       "3432312715945854947  Trump to order National Guard to protect borde...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create 'docs' column by combining title and text\n",
    "news_df['docs'] = news_df['title'] + ' ' + news_df['article']\n",
    "\n",
    "#filter for only necessary columns\n",
    "news_df = news_df[['date', 'publication', 'title', 'article', 'docs']]\n",
    "\n",
    "#discard any rows with missing values in these columns:\n",
    "news_df = news_df[news_df['date'].notna() & news_df['publication'].notna() & news_df['title'].notna() & news_df['article'].notna()]\n",
    "print(f\"Number of articles after dropping missing values: {len(news_df)}\")\n",
    "\n",
    "#keep only first 10 chars of date strings (drop time):\n",
    "news_df['date'] = news_df['date'].astype(str).str[:10]\n",
    "\n",
    "# Create unique ID for each article as hash of docs\n",
    "news_df['id'] = news_df['docs'].apply(lambda x: abs(hash(x)))\n",
    "news_df.set_index('id', inplace=True)\n",
    "news_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. load `models`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Device - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Set device preference order: GPU (CUDA) > MPS (ARM64) > CPU\n",
    "device = (torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "         else torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "         else torch.device(\"cpu\"))\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded spaCy model: en_core_web_lg\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Note: Select the appropriate spaCy model for your language.\n",
    "For languages other than English, ensure you have installed the corresponding model\n",
    "(e.g., 'nl_core_news_lg' for Dutch, 'de_core_news_lg' for German).\n",
    "See spaCy's model documentation for a complete list of available models.\n",
    "\"\"\"\n",
    "\n",
    "def load_spacy_model(model_name=\"en_core_web_lg\"):\n",
    "   \"\"\"\n",
    "   Load a spaCy model.\n",
    "   \n",
    "   Args:\n",
    "       model_name (str): Name of the spaCy model to load (default: \"en_core_web_lg\")\n",
    "                        Examples: \"nl_core_news_lg\", \"de_core_news_lg\"\n",
    "   \n",
    "   Returns:\n",
    "       spacy model: Loaded spaCy model\n",
    "   \"\"\"\n",
    "   try:\n",
    "       model = spacy.load(model_name)\n",
    "       print(f\"Loaded spaCy model: {model_name}\")\n",
    "       return model\n",
    "   except OSError as e:\n",
    "       raise OSError(f\"Model '{model_name}' not found. Make sure to install it first: \"\n",
    "                    f\"python -m spacy download {model_name}\")\n",
    "\n",
    "# Load the English model (change model_name for other languages)\n",
    "ner_model = load_spacy_model(model_name=\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Embedding - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded custom model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load a Sentence Transformer model for computing document embeddings.\n",
    "Either:\n",
    "\n",
    "1. Load the newsSimilarity model (default), which is optimized for article similarity\n",
    "  Note: Requires downloading the weights file 'state_dict.tar' from \n",
    "  https://huggingface.co/Blablablab/newsSimilarity\n",
    "2. Load any other model from the Sentence Transformers library.\n",
    "\n",
    "For available models, see:\n",
    "- https://huggingface.co/Blablablab/newsSimilarity (default newsSimilarity model)\n",
    "- https://www.sbert.net/docs/pretrained_models.html (other Sentence Transformer models)\n",
    "\"\"\"\n",
    "\n",
    "def load_sentence_transformer(model_name=None, weights_path=None, device=None):\n",
    "   \"\"\"\n",
    "   Load a Sentence Transformer model.\n",
    "   \n",
    "   Args:\n",
    "       model_name (str, optional): Name of specific model to load from Sentence Transformers.\n",
    "           If None, loads the newsSimilarity model (optimized for article similarity).\n",
    "       weights_path (str, optional): Path to newsSimilarity weights file (state_dict.tar).\n",
    "           Required if using the default newsSimilarity model.\n",
    "       device (torch.device, optional): Device to load the model on.\n",
    "           If None, uses the default device.\n",
    "   \n",
    "   Returns:\n",
    "       SentenceTransformer: Loaded model\n",
    "   \"\"\"\n",
    "   if model_name:\n",
    "       try:\n",
    "           model = SentenceTransformer(model_name, device=device)\n",
    "           print(f\"Loaded custom model: {model_name}\")\n",
    "       except Exception as e:\n",
    "           raise Exception(f\"Error loading model '{model_name}'. \"\n",
    "                         f\"Check if the model name is correct: {str(e)}\")\n",
    "   else:\n",
    "       if not weights_path:\n",
    "           raise ValueError(\"weights_path must be specified for newsSimilarity model. \"\n",
    "                          \"Download state_dict.tar from the model's HuggingFace page.\")\n",
    "       try:\n",
    "           # Load the base model\n",
    "           model = SentenceTransformer(\"all-mpnet-base-v2\", device=device)\n",
    "           \n",
    "           # Load the NewsSimilarity weights\n",
    "           state_dict = torch.load(weights_path, map_location=device)\n",
    "           \n",
    "           # Change naming convention to fit model\n",
    "           state_dict_new = {key.replace(\"model.\", \"\"): value for key, value in state_dict.items()}\n",
    "           if \"embeddings.position_ids\" in state_dict_new:\n",
    "               del state_dict_new[\"embeddings.position_ids\"]\n",
    "               \n",
    "           # Add weights to model\n",
    "           model._first_module().auto_model.load_state_dict(state_dict_new)\n",
    "           print(\"Loaded newsSimilarity model (Litterer et al. 2023)\")\n",
    "           \n",
    "       except Exception as e:\n",
    "           raise Exception(f\"Error loading newsSimilarity model: {str(e)}\")\n",
    "   \n",
    "   return model\n",
    "\n",
    "# Load newsSimilarity model (edit weights_path to location of state_dict.tar)\n",
    "# sent_model = load_sentence_transformer(weights_path=\"path/to/state_dict.tar\", device=device)\n",
    "\n",
    "# Or load a specified model from HuggingFace: \n",
    "sent_model = load_sentence_transformer(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. detect `named entities`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique news organizations: ['reuters', 'vox', 'vice', 'vice news', 'hyperallergic', 'tmz', 'business insider', 'techcrunch', 'axios', 'refinery 29', 'the verge', 'people', 'economist', 'mashable', 'cnn', 'gizmodo', 'wired', 'new republic', 'cnbc', 'the hill', 'politico', 'buzzfeed news', 'the new york times']\n"
     ]
    }
   ],
   "source": [
    "#generate a list of all unique values in 'publication' column (to filter out self-mentions from entities)\n",
    "publications = news_df['publication'].dropna().astype(str).str.lower().unique().tolist()\n",
    "print(f\"unique news organizations: {publications}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2035/2035 [02:54<00:00, 11.68it/s]\n"
     ]
    }
   ],
   "source": [
    "def detect_entities(news_df, spacy_model, news_orgs, entity_types=None):\n",
    "   \"\"\"\n",
    "   Perform Named Entity Recognition using spaCy.\n",
    "   \n",
    "   Args:\n",
    "       news_df (pd.DataFrame): DataFrame containing documents in 'docs' column\n",
    "       spacy_model: Loaded spaCy model\n",
    "       news_orgs (list): list of news organizations to exclude from entities.\n",
    "       entity_types (list, optional): List of entity types to extract.\n",
    "           If None, uses default entities (see below).\n",
    "           \n",
    "   Default entity types included:\n",
    "       PERSON: People, including fictional \n",
    "       ORG: Companies, agencies, institutions\n",
    "       NORP: Nationalities, religious or political groups\n",
    "       GPE: Countries, cities, states\n",
    "       PRODUCT: Products, objects, vehicles, foods, etc.\n",
    "       EVENT: Named events like wars, sports events, hurricanes\n",
    "       WORK_OF_ART: Titles of books, songs, etc.\n",
    "       LAW: Named documents made into laws\n",
    "       FAC: Buildings, airports, highways, bridges\n",
    "       LOC: Non-GPE locations, mountain ranges, bodies of water\n",
    "       \n",
    "   Entity types excluded by default:\n",
    "       Numerical and temporal entities (DATE, TIME, PERCENT, MONEY, QUANTITY, ORDINAL, CARDINAL)\n",
    "       and LANGUAGE are excluded as they often represent incidental details in news articles\n",
    "       rather than core topical content. For example, two articles about the same event\n",
    "       may use different dates or monetary figures while covering the same core story.\n",
    "   \n",
    "   Returns:\n",
    "       pd.DataFrame: DataFrame with added 'spacy_entities' column containing\n",
    "                    list of tuples (entity_text, entity_label, entity_vector)\n",
    "   \"\"\"\n",
    "   # Default entity types excl. numerical/temporal.\n",
    "   DEFAULT_ENTITY_TYPES = [\n",
    "       'PERSON', 'ORG', 'NORP', 'GPE', 'PRODUCT', \n",
    "       'EVENT', 'WORK_OF_ART', 'LAW', 'FAC', 'LOC'\n",
    "   ]\n",
    "   \n",
    "   entity_types = entity_types or DEFAULT_ENTITY_TYPES\n",
    "   \n",
    "   def extract_entities_spacy(text):\n",
    "       try:\n",
    "           spacy_doc = spacy_model(text)\n",
    "           entities = [\n",
    "               (ent.text, ent.label_)\n",
    "               for ent in spacy_doc.ents\n",
    "               if ent.label_ in entity_types\n",
    "               and ent.text.lower() not in news_orgs\n",
    "           ]\n",
    "           return entities\n",
    "           \n",
    "       except Exception as e:\n",
    "           print(f\"Error processing text with spaCy: {e}\")\n",
    "           return []\n",
    "   \n",
    "   # Apply NER with progress bar\n",
    "   tqdm.pandas()\n",
    "   news_df['entities'] = news_df['docs'].progress_apply(extract_entities_spacy)\n",
    "   \n",
    "   return news_df\n",
    "\n",
    "news_df = detect_entities(news_df, ner_model, news_orgs=publications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. `document embeddings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating document embeddings using SentenceTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:15<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "def calculate_doc_embeddings(news_df, sent_model, batch_size=32):\n",
    "   \"\"\"\n",
    "   Calculate document embeddings for the 'docs' column using a sentence transformer model.\n",
    "   Optimized for speed through batched processing.\n",
    "   \n",
    "   Args:\n",
    "       news_df (pd.DataFrame): DataFrame containing documents in 'docs' column\n",
    "       sent_model (SentenceTransformer): Loaded sentence transformer model\n",
    "       batch_size (int): Number of documents to embed at once (default: 32)\n",
    "   \n",
    "   Returns:\n",
    "       pd.DataFrame: Input DataFrame with added 'document_embedding' column\n",
    "                    containing document embeddings as numpy arrays\n",
    "   \"\"\"\n",
    "   print(f\"Calculating document embeddings using {sent_model.__class__.__name__}\")\n",
    "   \n",
    "   # Process documents in batches\n",
    "   all_embeddings = []\n",
    "   for i in tqdm(range(0, len(news_df), batch_size)):\n",
    "       batch_texts = news_df['docs'].iloc[i:i + batch_size].tolist()\n",
    "       try:\n",
    "           # Calculate embeddings for batch\n",
    "           embeddings = sent_model.encode(batch_texts, convert_to_tensor=True)\n",
    "           \n",
    "           # Convert to numpy and store\n",
    "           all_embeddings.extend(embeddings.cpu().numpy())\n",
    "       except Exception as e:\n",
    "           print(f\"Error processing batch {i//batch_size}: {e}\")\n",
    "           \n",
    "           # If error, pad with zeros for this batch\n",
    "           embedding_dim = sent_model.get_sentence_embedding_dimension()\n",
    "           all_embeddings.extend([np.zeros(embedding_dim) for _ in batch_texts])\n",
    "   \n",
    "   news_df['document_embedding'] = all_embeddings\n",
    "   return news_df\n",
    "\n",
    "news_df = calculate_doc_embeddings(news_df, sent_model, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. `subject/context embeddings`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_subject_context_embeddings(news_df, sent_model, pos_tagging=\"en_core_web_lg\", stop_lang = 'english', sub_ngram=(1,3), sub_topN=10, con_topN=10):\n",
    "    \n",
    "    \"\"\"\n",
    "Extract and calculate subject and context embeddings for documents using KeyBERT.\n",
    "\n",
    "This function performs two types of keyphrase extraction and embedding calculation:\n",
    "\n",
    "1. Subject Embeddings: Based on named entities in the document.\n",
    "  - Extracts keyphrases based on candidate list of named entities.\n",
    "  - Calculates salience-weighted embeddings of these entities.\n",
    "  - Represents the \"about whom\" of the document\n",
    "\n",
    "2. Context Embeddings: Based on non-entity noun-centric phrases.\n",
    "  - Extracts keyphrases using POS patterns (e.g., noun+verb, adj+noun)\n",
    "  - Calculates similarity-weighted embeddings of these contextual phrases\n",
    "  - Represents the \"about what\" of the document\n",
    "\n",
    "Args:\n",
    "   news_df (pd.DataFrame): DataFrame containing:\n",
    "       - 'docs' column with document texts (title + text).\n",
    "       - 'entities' column with lists of extracted (entity_text, entity_label) tuples.\n",
    "   sent_model: Sentence transformer model for embeddings\n",
    "   pos_tagging (str): Name of spaCy model for POS tagging (default: \"en_core_web_lg\")\n",
    "   stop_lang (str): Language for stopwords from NLTK (default: \"english\")\n",
    "   sub_ngram (tuple): N-gram range for subject keyphrases (default: (1,3))\n",
    "   NOTE: ngram range for context keyphrases is dynamic (see KeyphraseCountVectorizer docs)\n",
    "   sub_topN (int): Number of top subject keyphrases to keep (default: 10)\n",
    "   con_topN (int): Number of top context keyphrases to keep (default: 10)\n",
    "\n",
    "Returns:\n",
    "   pd.DataFrame: Input DataFrame with added columns:\n",
    "       - subject_keywords: List of extracted subject keyphrases\n",
    "       - context_keywords: List of extracted context keyphrases\n",
    "       - subject_weights: Corresponding weights for subject keyphrases\n",
    "       - context_weights: Corresponding weights for context keyphrases\n",
    "       - subject_embedding: Weighted average embedding of subject keyphrases\n",
    "       - context_embedding: Weighted average embedding of context keyphrases\n",
    "\n",
    "Note:\n",
    "   - Requires KeyBERT package.\n",
    "   - Requires KeyphraseCountVectorizer package.\n",
    "\"\"\"\n",
    "    print(\"--> Running calculate_subject_embedding()...\")\n",
    "\n",
    "    #- - - PREPARATION - - - >>>\n",
    "\n",
    "    #get list of texts: \n",
    "    docs = news_df['docs'].to_list()\n",
    "\n",
    "    #pass the ST model to keyBERT:\n",
    "    kw_model = KeyBERTEmbeddings(model=sent_model)\n",
    "\n",
    "    #create output lists for the dataframe:\n",
    "    subject_keywords_master, context_keywords_master = [], []\n",
    "    subject_weights_master, context_weights_master = [], []\n",
    "    subject_embedding_master, context_embedding_master = [], []\n",
    "\n",
    "    #generate stop word list (pass language code):\n",
    "    stop_words = list(set(nltk.corpus.stopwords.words(stop_lang)))\n",
    "\n",
    "    #generate subject candidate list of all relevant entities accross all articles (lowercased):\n",
    "    #NOTE: we drop stopwords and single character entities from the candidate list (in case these made it through). \n",
    "    subject_candidates = list(set([entity[0].strip().lower() for entities in news_df['entities'] for entity in entities]))\n",
    "    subject_candidates = [candidate for candidate in subject_candidates if candidate not in stop_words] # - stopwords\n",
    "    subject_candidates = [candidate for candidate in subject_candidates if len(candidate) > 1] # - single characters\n",
    "\n",
    "    #define a helper function to de-duplicate keyphrases, average their embeddings, and sum their weights:\n",
    "    def deduplicate_and_sum(keywords):\n",
    "        grouped = defaultdict(lambda: [0, []])\n",
    "        for keyword, score, embedding in keywords:\n",
    "            grouped[keyword][0] += score  \n",
    "            grouped[keyword][1].append(embedding)\n",
    "        return [(k, total_score, np.mean(embeddings, axis=0)) \n",
    "                for k, (total_score, embeddings) in grouped.items()]\n",
    "    \n",
    "    #define a helpter function to handle skips:\n",
    "    def skip_article(index, reason):\n",
    "        print(f\"Skipping article at index {index}: {reason}\")\n",
    "        subject_embedding_master.append(None)\n",
    "        subject_keywords_master.append(None)\n",
    "        subject_weights_master.append(None)\n",
    "        context_weights_master.append(None)\n",
    "        context_embedding_master.append(None)\n",
    "        context_keywords_master.append(None)\n",
    "\n",
    "    \n",
    "    #- - - COUNT VECTORIZER SETUP - - - >>>\n",
    "\n",
    "    #define a KeyphraseCountVectorizer for context keyphrase extraction:\n",
    "    con_vectorizer = KeyphraseCountVectorizer(spacy_pipeline=pos_tagging, stop_words=stop_words, \n",
    "                                                  lowercase=True, pos_pattern='(<J.*>*<N.*>+)|(<N.*>+<V.*>)|(<V.*><N.*>+)|(<N.*>+<IN><N.*>+)')\n",
    "\n",
    "    #NOTE: different POS patterns can be used to extract different types of keyphrases. For example, here:\n",
    "    #noun, noun+verb, adj+noun, verb+noun, noun+prep+noun = (<J.*>*<N.*>+)|(<N.*>+<V.*>)|(<V.*><N.*>+)|(<N.*>+<IN><N.*>+)\n",
    "    \n",
    "\n",
    "\n",
    "    #- - - SUBJECT KEYPHRASE EXTRACTION - - - >>>\n",
    "\n",
    "    #generate kw embeddings for SUBJECT (by passing subject entities as candidate list):\n",
    "    print(f\"--> extracting subject embeddings...\")\n",
    "    subject_doc_embeds, subject_kw_embeds = kw_model.extract_embeddings(docs, \n",
    "                                                                        keyphrase_ngram_range=sub_ngram, \n",
    "                                                                        candidates=subject_candidates,\n",
    "                                                                        )\n",
    "\n",
    "    #extract topN keywords from the subject keyword embeddings (keyword, similarity, embedding):\n",
    "    print(f\"--> extracting subject keyphrases...\")\n",
    "    news_df['subject_keywords'] = kw_model.extract_keywords(docs, top_n=sub_topN,\n",
    "                                                            keyphrase_ngram_range=sub_ngram, \n",
    "                                                            candidates=subject_candidates,\n",
    "                                                            word_embeddings=subject_kw_embeds,\n",
    "                                                            doc_embeddings=subject_doc_embeds,\n",
    "                                                            )\n",
    "    \n",
    "\n",
    "\n",
    "    #- - - CONTEXT KEYPHRASE EXTRACTION - - - >>>\n",
    "\n",
    "    #generate kw_embeddings for CONTEXT (passing customized count vectorizer):\n",
    "    print(f\"--> extracting context embeddings...\")\n",
    "    context_doc_embed, context_kw_embeds = kw_model.extract_embeddings(docs, \n",
    "                                                                        vectorizer=con_vectorizer)\n",
    "    \n",
    "    #generate context keywords for the articles:\n",
    "    print(f\"--> extracting context keyphrases...\")\n",
    "    news_df['context_keywords'] = kw_model.extract_keywords(docs, top_n=con_topN,\n",
    "                                                vectorizer=con_vectorizer,\n",
    "                                                word_embeddings=context_kw_embeds,\n",
    "                                                doc_embeddings=context_doc_embed,\n",
    "                                                )\n",
    "\n",
    "    \n",
    "    #- - - CALCULATE SUBJECT/CONTEXT EMBEDDINGS --- >>>\n",
    "    print(\"--> Calculating subject/context embeddings...\")\n",
    "    \n",
    "    #create a list to store indices of failed rows:\n",
    "    failed_indices = []\n",
    "    \n",
    "    for index, row in tqdm(news_df.iterrows()):\n",
    "        \n",
    "        #SUBJECT EMBEDDING -\n",
    "        #NOTE: defined as salience-weighted embedding of entities in the article.\n",
    "        try:\n",
    "            #get the subject keyphrases for the article:\n",
    "            subject_keywords = row['subject_keywords']\n",
    "            \n",
    "            #[CHECK] if no subject keywords detected, skip this article:\n",
    "            if not subject_keywords or len(subject_keywords) == 0:\n",
    "                print(f\"No subject keywords detected for article at index {index}. Skipping...\")\n",
    "                failed_indices.append(index)\n",
    "                subject_keywords_master.append(None) \n",
    "                subject_weights_master.append(None)\n",
    "                subject_embedding_master.append(None)\n",
    "                context_keywords_master.append(None)\n",
    "                context_weights_master.append(None) \n",
    "                context_embedding_master.append(None)\n",
    "                continue\n",
    "            \n",
    "            #de-duplicate subject keyphrases (sum scores, avg embeds):\n",
    "            subject_keywords = deduplicate_and_sum(subject_keywords)\n",
    "            #sort the subject_keywords by weights (second element of the tuple)\n",
    "            subject_keywords = sorted(subject_keywords, key=lambda x: x[1], reverse=True) \n",
    "            #[FILTER] keep only the topN subject keyphrases:\n",
    "            subject_keywords = subject_keywords[:sub_topN]\n",
    "            #unpack the subject keyphrase tuples for the article:\n",
    "            subject_keywords, subject_weights, subject_embeddings = zip(*subject_keywords)\n",
    "            #calculate the subject embedding as the salience-weighted average of the embeddings:\n",
    "            subject_embedding = np.average(subject_embeddings, axis=0, weights=subject_weights)\n",
    "            \n",
    "            #append subject data to master lists:\n",
    "            subject_keywords_master.append(subject_keywords) \n",
    "            subject_weights_master.append(subject_weights)\n",
    "            subject_embedding_master.append(subject_embedding)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing subject embedding for article at index {index}: {str(e)}\")\n",
    "            failed_indices.append(index)\n",
    "            subject_keywords_master.append(None) \n",
    "            subject_weights_master.append(None)\n",
    "            subject_embedding_master.append(None)\n",
    "            #skip context processing for this article:\n",
    "            context_keywords_master.append(None)\n",
    "            context_weights_master.append(None) \n",
    "            context_embedding_master.append(None)\n",
    "            continue\n",
    "\n",
    "            \n",
    "        #CONTEXT EMBEDDDING - \n",
    "        #NOTE: doc-similarity weighted embedding of non-entity noun-centric keyphrases in the article.   \n",
    "        try:\n",
    "            #get the context keyphrases for the article:\n",
    "            context_keywords = row['context_keywords']\n",
    "            \n",
    "            #[CHECK] if no context keywords detected, skip this article:\n",
    "            if not context_keywords or len(context_keywords) == 0:\n",
    "                print(f\"No context keywords detected for article at index {index}. Skipping...\")\n",
    "                failed_indices.append(index)\n",
    "                context_keywords_master.append(None)\n",
    "                context_weights_master.append(None) \n",
    "                context_embedding_master.append(None)\n",
    "                continue\n",
    "            \n",
    "            #de-duplicate keyphrases (sum scores, avg embeds):\n",
    "            context_keywords = deduplicate_and_sum(context_keywords)\n",
    "            #sort the context_keywords by weights (second element of the tuple)\n",
    "            context_keywords = sorted(context_keywords, key=lambda x: x[1], reverse=True)\n",
    "            #[FILTER] discard context keyphrases that contain entities from the subject keyphrases:\n",
    "            context_keywords = [(context, score, embedding) for context, score, embedding in context_keywords if not any(word in subject_keywords for word in context.lower().split())]\n",
    "            #[FILTER] discard context keyphrases that are single characters:\n",
    "            context_keywords = [(context, score, embedding) for context, score, embedding in context_keywords if len(context) > 1]\n",
    "            \n",
    "            #[CHECK] if no context keywords remain after filtering, skip this article:\n",
    "            if not context_keywords or len(context_keywords) == 0:\n",
    "                print(f\"No context keywords remain after filtering for article at index {index}. Skipping...\")\n",
    "                failed_indices.append(index)\n",
    "                context_keywords_master.append(None)\n",
    "                context_weights_master.append(None) \n",
    "                context_embedding_master.append(None)\n",
    "                continue\n",
    "            \n",
    "            #[FILTER]keep only the topN resulting keyphrases:\n",
    "            context_keywords = context_keywords[:con_topN]\n",
    "            #unpack the context keyphrase tuples for the article:\n",
    "            context_keywords, context_weights, context_embedding = zip(*context_keywords)\n",
    "            #calculate the context embedding as the salience-weighted average of the embeddings:\n",
    "            context_embedding = np.average(context_embedding, axis=0, weights=context_weights)\n",
    "            \n",
    "            #append context data to master lists:\n",
    "            context_keywords_master.append(context_keywords)\n",
    "            context_weights_master.append(context_weights) \n",
    "            context_embedding_master.append(context_embedding)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing context embedding for article at index {index}: {str(e)}\")\n",
    "            failed_indices.append(index)\n",
    "            context_keywords_master.append(None)\n",
    "            context_weights_master.append(None) \n",
    "            context_embedding_master.append(None)\n",
    "            continue\n",
    "        \n",
    "\n",
    "    #- - -FINALIZATION - - - >>>\n",
    "\n",
    "    #merge the subject and context embeddings into the dataframe:\n",
    "    news_df['subject_keywords'] = subject_keywords_master\n",
    "    news_df['context_keywords'] = context_keywords_master\n",
    "    news_df['subject_weights'] = subject_weights_master\n",
    "    news_df['context_weights'] = context_weights_master\n",
    "    news_df['subject_embedding'] = subject_embedding_master\n",
    "    news_df['context_embedding'] = context_embedding_master\n",
    "\n",
    "    #remove rows that failed during processing:\n",
    "    if failed_indices:\n",
    "        print(f\"\\n--> Removing {len(failed_indices)} failed articles from the dataframe...\")\n",
    "        news_df = news_df.drop(index=failed_indices)\n",
    "\n",
    "    return news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Running calculate_subject_embedding()...\n",
      "--> extracting subject embeddings...\n",
      "--> extracting subject keyphrases...\n",
      "--> extracting context embeddings...\n",
      "--> extracting context keyphrases...\n",
      "--> Calculating subject/context embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1203it [00:00, 6071.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No context keywords remain after filtering for article at index 975812754316075345. Skipping...\n",
      "No context keywords remain after filtering for article at index 3367515107652030157. Skipping...\n",
      "No context keywords remain after filtering for article at index 2327374466068382576. Skipping...\n",
      "No context keywords remain after filtering for article at index 9008062070283884736. Skipping...\n",
      "No context keywords remain after filtering for article at index 9066736986716052601. Skipping...\n",
      "No context keywords remain after filtering for article at index 4623124509149724831. Skipping...\n",
      "No context keywords remain after filtering for article at index 7817694389214201856. Skipping...\n",
      "No context keywords remain after filtering for article at index 5552911458848226768. Skipping...\n",
      "No context keywords remain after filtering for article at index 6748957417935916943. Skipping...\n",
      "No context keywords remain after filtering for article at index 8736714236289197605. Skipping...\n",
      "No context keywords remain after filtering for article at index 8717280185496251970. Skipping...\n",
      "No context keywords remain after filtering for article at index 1477128211062124474. Skipping...\n",
      "No context keywords remain after filtering for article at index 455351816591680110. Skipping...\n",
      "No context keywords remain after filtering for article at index 6673983044522989755. Skipping...\n",
      "No context keywords remain after filtering for article at index 5002384202237759918. Skipping...\n",
      "No context keywords remain after filtering for article at index 7109401391561975592. Skipping...\n",
      "No context keywords remain after filtering for article at index 1488262848506678462. Skipping...\n",
      "No context keywords remain after filtering for article at index 941115763203361360. Skipping...\n",
      "No context keywords remain after filtering for article at index 8371988515639865033. Skipping...\n",
      "No context keywords remain after filtering for article at index 5998332614539593148. Skipping...\n",
      "No context keywords remain after filtering for article at index 2315198312882744708. Skipping...\n",
      "No context keywords remain after filtering for article at index 2671271612277613070. Skipping...\n",
      "No context keywords remain after filtering for article at index 5249132861900929525. Skipping...\n",
      "No context keywords remain after filtering for article at index 1367963357940442070. Skipping...\n",
      "No context keywords remain after filtering for article at index 7468080882874272488. Skipping...\n",
      "No context keywords remain after filtering for article at index 2298368157347157330. Skipping...\n",
      "No context keywords remain after filtering for article at index 6330682767053819975. Skipping...\n",
      "No context keywords remain after filtering for article at index 680487008637711821. Skipping...\n",
      "No context keywords remain after filtering for article at index 4668520605834981841. Skipping...\n",
      "No context keywords remain after filtering for article at index 108625323677984390. Skipping...\n",
      "No context keywords remain after filtering for article at index 8260040476898600043. Skipping...\n",
      "No context keywords remain after filtering for article at index 1554826389645957445. Skipping...\n",
      "No context keywords remain after filtering for article at index 210949877160903890. Skipping...\n",
      "No context keywords remain after filtering for article at index 7123535298424080787. Skipping...\n",
      "No context keywords remain after filtering for article at index 8510837120228578859. Skipping...\n",
      "No context keywords remain after filtering for article at index 6723776269382937621. Skipping...\n",
      "No context keywords remain after filtering for article at index 8753028206627203126. Skipping...\n",
      "No context keywords remain after filtering for article at index 7880494405705235768. Skipping...\n",
      "No context keywords remain after filtering for article at index 4261125052852552116. Skipping...\n",
      "No context keywords remain after filtering for article at index 3723559713339675948. Skipping...\n",
      "No context keywords remain after filtering for article at index 4655021911091571473. Skipping...\n",
      "No context keywords remain after filtering for article at index 128027316544655588. Skipping...\n",
      "No context keywords remain after filtering for article at index 8757923287891192921. Skipping...\n",
      "No context keywords remain after filtering for article at index 8990311389369661775. Skipping...\n",
      "No context keywords remain after filtering for article at index 7329174044328739260. Skipping...\n",
      "No context keywords remain after filtering for article at index 6078120782816004408. Skipping...\n",
      "No context keywords remain after filtering for article at index 2836323102022226797. Skipping...\n",
      "No context keywords remain after filtering for article at index 219415874628240391. Skipping...\n",
      "No context keywords remain after filtering for article at index 3516974666626291774. Skipping...\n",
      "No context keywords remain after filtering for article at index 961727667927834232. Skipping...\n",
      "No context keywords remain after filtering for article at index 3105250618969103347. Skipping...\n",
      "No context keywords remain after filtering for article at index 4655021911091571473. Skipping...\n",
      "No context keywords remain after filtering for article at index 8975072428688842880. Skipping...\n",
      "No context keywords remain after filtering for article at index 3723559713339675948. Skipping...\n",
      "No context keywords remain after filtering for article at index 6411824401919512227. Skipping...\n",
      "No context keywords remain after filtering for article at index 8416395930843300463. Skipping...\n",
      "No context keywords remain after filtering for article at index 2836323102022226797. Skipping...\n",
      "No context keywords remain after filtering for article at index 4270959546402511869. Skipping...\n",
      "No context keywords remain after filtering for article at index 1143412003976227919. Skipping...\n",
      "No context keywords remain after filtering for article at index 7091878570100463685. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2035it [00:00, 5779.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No context keywords remain after filtering for article at index 8589980233990383855. Skipping...\n",
      "No context keywords remain after filtering for article at index 301532724649891210. Skipping...\n",
      "No context keywords remain after filtering for article at index 145011995044705850. Skipping...\n",
      "No context keywords remain after filtering for article at index 8398402481593308122. Skipping...\n",
      "No context keywords remain after filtering for article at index 1318342818413085111. Skipping...\n",
      "No context keywords remain after filtering for article at index 6239220938549572795. Skipping...\n",
      "No context keywords remain after filtering for article at index 9222273047746770552. Skipping...\n",
      "No context keywords remain after filtering for article at index 6107595644643456704. Skipping...\n",
      "No context keywords remain after filtering for article at index 6276321503152423197. Skipping...\n",
      "No context keywords remain after filtering for article at index 5225548428061178675. Skipping...\n",
      "No context keywords remain after filtering for article at index 656987603043788986. Skipping...\n",
      "No context keywords remain after filtering for article at index 4299987023077044428. Skipping...\n",
      "No context keywords remain after filtering for article at index 2396051006386364412. Skipping...\n",
      "No context keywords remain after filtering for article at index 8276857133428889789. Skipping...\n",
      "No context keywords remain after filtering for article at index 7102252730535178512. Skipping...\n",
      "No context keywords remain after filtering for article at index 2000849578414518220. Skipping...\n",
      "No context keywords remain after filtering for article at index 6098066036845453732. Skipping...\n",
      "No context keywords remain after filtering for article at index 86939714658730938. Skipping...\n",
      "No context keywords remain after filtering for article at index 6825911167777657869. Skipping...\n",
      "No context keywords remain after filtering for article at index 90653116304724136. Skipping...\n",
      "No context keywords remain after filtering for article at index 6535327103141773065. Skipping...\n",
      "No context keywords remain after filtering for article at index 9014504631731415927. Skipping...\n",
      "No context keywords remain after filtering for article at index 6828873294575606478. Skipping...\n",
      "No context keywords remain after filtering for article at index 1307932630099630354. Skipping...\n",
      "No context keywords remain after filtering for article at index 4022017111283681270. Skipping...\n",
      "No context keywords remain after filtering for article at index 1338569988457185072. Skipping...\n",
      "No context keywords remain after filtering for article at index 3566982623597387472. Skipping...\n",
      "No context keywords remain after filtering for article at index 458193266305963681. Skipping...\n",
      "No context keywords remain after filtering for article at index 1700142588429434830. Skipping...\n",
      "No context keywords remain after filtering for article at index 6020839761518542419. Skipping...\n",
      "No context keywords remain after filtering for article at index 3719683137147278750. Skipping...\n",
      "No context keywords remain after filtering for article at index 6922448523909989842. Skipping...\n",
      "No context keywords remain after filtering for article at index 6629162647998871046. Skipping...\n",
      "No context keywords remain after filtering for article at index 3165547555972669783. Skipping...\n",
      "No context keywords remain after filtering for article at index 3597124021889165040. Skipping...\n",
      "No context keywords remain after filtering for article at index 1403881049061510915. Skipping...\n",
      "No context keywords remain after filtering for article at index 2972078273738714869. Skipping...\n",
      "No context keywords remain after filtering for article at index 7055472976338667271. Skipping...\n",
      "No context keywords remain after filtering for article at index 1099743091068482768. Skipping...\n",
      "No context keywords remain after filtering for article at index 5703018937739125639. Skipping...\n",
      "No context keywords remain after filtering for article at index 4517432816866108451. Skipping...\n",
      "No context keywords remain after filtering for article at index 5233843315687939656. Skipping...\n",
      "\n",
      "--> Removing 102 failed articles from the dataframe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "news_df = calculate_subject_context_embeddings(news_df, sent_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. `event clustering` (Litterer et al. 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >> for original implementation, see: https://github.com/blitt2018/mediaStorms\n",
    "\n",
    "###### >> this is an integration of the following scripts in the above workflow:  `0.0-bl-getEntityPairs.py` + `0.1-bl-computeCosineSim.py` + `0.2-bl-createEmbeddingClusterList.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVENT DETECTION / CLUSTERING -\n",
    "SIM_CUTOFF = 0.8 #minimum similarity for clustering (cosine similarity)\n",
    "CLUSTER_CUTOFF = [2, 100] #minimum and maximum cluster size (e.g, [2, 100] means min 2 docs, max 100 docs per cluster)\n",
    "included_entity_types = ['PERSON', 'ORG', 'NORP', 'GPE', 'PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW', 'FAC', 'LOC'] \n",
    "excluded_entity_types = ['LANGUAGE','DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(news_df: pd.DataFrame, CLUSTER_CUTOFF: tuple, SIM_CUTOFF: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Cluster news articles based on named entities and document embeddings similarity.\n",
    "    \n",
    "    Args:\n",
    "        news_df: DataFrame containing news articles with entities and embeddings\n",
    "        CLUSTER_CUTOFF: Tuple of (min_size, max_size) for filtering clusters\n",
    "        SIM_CUTOFF: Minimum cosine similarity threshold for clustering\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (DataFrame with cluster assignments, NetworkX graph of article relationships)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Helper Functions\n",
    "    def getPairwise(inList): \n",
    "        \"\"\"\n",
    "        Generate all unique pairwise combinations of elements within a list.\n",
    "        \n",
    "        Args:\n",
    "            inList: List of elements to generate pairs from\n",
    "        Returns:\n",
    "            List of paired elements\n",
    "        \"\"\"\n",
    "        outList = []\n",
    "        inLen = len(inList)\n",
    "        for i in range(0, inLen):\n",
    "            for j in range(i+1, inLen): \n",
    "                outList.append([inList[i], inList[j]])\n",
    "        return outList\n",
    "\n",
    "    def getCos(inList): \n",
    "        \"\"\"\n",
    "        Calculate cosine similarity between two document embedding vectors.\n",
    "        \n",
    "        Args:\n",
    "            inList: List/array containing exactly two vectors\n",
    "        Returns:\n",
    "            float: Cosine similarity score\n",
    "        \"\"\"\n",
    "        if not isinstance(inList, (list, np.ndarray)) or len(inList) != 2:\n",
    "            raise ValueError(\"Input must be list/array of two vectors\")\n",
    "        return float(cosine_similarity(inList[0].reshape(1, -1), \n",
    "                                    inList[1].reshape(1, -1))[0][0])\n",
    "\n",
    "    def getCosSeries(inSeries): \n",
    "        \"\"\"\n",
    "        Apply cosine similarity calculation to a series of vector pairs.\n",
    "        \n",
    "        Args:\n",
    "            inSeries: Pandas Series containing pairs of vectors\n",
    "        Returns:\n",
    "            Series of similarity scores\n",
    "        \"\"\"\n",
    "        if not isinstance(inSeries, pd.Series):\n",
    "            raise TypeError(\"Input must be pandas Series\")\n",
    "        return inSeries.apply(getCos)\n",
    "\n",
    "    # STEP 1: Data Preparation\n",
    "    # Extract relevant columns and explode entity information\n",
    "    lean_df = news_df[['entities','document_embedding']].reset_index(drop=False).copy()\n",
    "    lean_df = lean_df.explode('entities')\n",
    "    lean_df[['entity','ent_type']] = pd.DataFrame(lean_df['entities'].tolist(), index=lean_df.index)\n",
    "    \n",
    "    # Remove duplicate entity mentions within same article\n",
    "    lean_df = lean_df.drop_duplicates(subset=[\"id\", \"entity\", \"ent_type\"])\n",
    "\n",
    "    # STEP 2: Group and Filter Articles\n",
    "    # Group articles by named entities and apply size filters\n",
    "    grouped_df = lean_df[[\"ent_type\", \"entity\", \"id\", \"document_embedding\"]].groupby(by=[\"ent_type\", \"entity\"]).agg(list)\n",
    "    grouped_df[\"numArticles\"] = grouped_df[\"id\"].apply(len)\n",
    "    grouped_df = grouped_df[(grouped_df[\"numArticles\"] >= CLUSTER_CUTOFF[0]) & \n",
    "                          (grouped_df[\"numArticles\"] <= CLUSTER_CUTOFF[1])]\n",
    "    \n",
    "    # STEP 3: Generate Article Pairs\n",
    "    # Create pairs of articles and their embeddings for similarity calculation\n",
    "    grouped_df = grouped_df[[\"id\", \"document_embedding\"]]\n",
    "    grouped_df[\"document_embedding\"] = grouped_df[\"document_embedding\"].apply(getPairwise)\n",
    "    grouped_df[\"id\"] = grouped_df[\"id\"].apply(getPairwise)\n",
    "    \n",
    "    # STEP 4: Calculate Similarities\n",
    "    # Process pairs and calculate similarity scores\n",
    "    pair_df = grouped_df.apply(pd.Series.explode)\n",
    "    pair_df[[\"id1\", \"id2\"]] = pd.DataFrame(pair_df[\"id\"].to_list(), index=pair_df.index)\n",
    "    pair_df = pair_df.drop(columns=[\"id\"]).reset_index()\n",
    "    pair_df = pair_df.drop_duplicates(subset=[\"id1\", \"id2\"]).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate and filter by similarity threshold\n",
    "    embeddings = pair_df[\"document_embedding\"]\n",
    "    similarity = getCosSeries(embeddings)\n",
    "    pair_df[\"similarity\"] = similarity\n",
    "    pair_df = pair_df[pair_df[\"similarity\"] >= SIM_CUTOFF].reset_index(drop=True)\n",
    "\n",
    "    # STEP 5: Generate Clusters\n",
    "    # Create graph and identify connected components\n",
    "    graph = nx.from_pandas_edgelist(pair_df[[\"id1\", \"id2\"]], \"id1\", \"id2\")\n",
    "    components = nx.connected_components(graph)\n",
    "    comp_list = [comp for comp in components]\n",
    "\n",
    "    # STEP 6: Merge Results\n",
    "    # Convert clusters to DataFrame and merge with original data\n",
    "    clusters = pd.DataFrame({\"cluster\":comp_list}).reset_index()\n",
    "    clust_df = clusters.explode(\"cluster\").rename(columns={\"index\":\"clustNum\", \"cluster\":\"id\"})\n",
    "    news_df = news_df.merge(clust_df, left_index=True, right_on=\"id\", how=\"left\").set_index(\"id\")\n",
    "\n",
    "    # Print clustering statistics\n",
    "    cluster_sizes = clust_df['clustNum'].value_counts()\n",
    "    size_counts = cluster_sizes.value_counts().sort_index()\n",
    "    \n",
    "    print(f\"Number of clusters: {len(clust_df['clustNum'].unique())}\")\n",
    "    print(f\"Number of articles in clusters: {len(clust_df)} of {len(news_df)} total articles.\")\n",
    "    print(\"Number of clusters by size:\")\n",
    "    for size, count in size_counts.items():\n",
    "        print(f\"  - {count} clusters with {size} articles\")\n",
    "\n",
    "    return news_df, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 140\n",
      "Number of articles in clusters: 509 of 1933 total articles.\n",
      "Number of clusters by size:\n",
      "  - 85 clusters with 2 articles\n",
      "  - 36 clusters with 3 articles\n",
      "  - 5 clusters with 4 articles\n",
      "  - 2 clusters with 5 articles\n",
      "  - 4 clusters with 6 articles\n",
      "  - 1 clusters with 7 articles\n",
      "  - 2 clusters with 8 articles\n",
      "  - 1 clusters with 11 articles\n",
      "  - 1 clusters with 12 articles\n",
      "  - 1 clusters with 25 articles\n",
      "  - 1 clusters with 38 articles\n",
      "  - 1 clusters with 68 articles\n"
     ]
    }
   ],
   "source": [
    "news_df, graph = get_clusters(news_df, CLUSTER_CUTOFF, SIM_CUTOFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. save to disk: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump the data to a .csv file: \n",
    "news_df.to_csv('./data/01_processing_output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnrs_algorithm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
